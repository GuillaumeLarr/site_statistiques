[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce document compile mes notes au cours de la formation « data analyst » proposée par le CEPE (ENSAI-ENSAE)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#contenu-de-la-formation",
    "href": "index.html#contenu-de-la-formation",
    "title": "Introduction",
    "section": "Contenu de la formation",
    "text": "Contenu de la formation\n\nStatistiques descriptives\nStatistique inférentielle\n\nProbabilités\nIntervalles de confiance\nTest statistique\n\nAnalyse factorielle\n\nAnalyse en composantes principales (ACP)\nAnalyse factorielle des correspondances (AFC)\nAnalyse des correspondances multiples (ACM)\n\nRégression linéaire\n\nSimple\nMultiple\n\nClassification supervisée\n\nRégression logistique\nArbres\n\nClassifications non-supervisée\n\nMéthodes de partitionnement (k-means)\nClassification ascendante hiérarchique (CAH)\n\nIntroduction au text mining"
  },
  {
    "objectID": "index.html#références-bibliographiques",
    "href": "index.html#références-bibliographiques",
    "title": "Introduction",
    "section": "Références bibliographiques",
    "text": "Références bibliographiques\n\nStatistique descriptive, cours et exercices corrigés - A.Hamon, N.Jégou - PUR (2008)\nStatistique générale pour utilisateurs - J.Pagès - PUR (2012)\nStatistique : économie-gestion-sciences-médecine - H.Wonnacott, J.Wonnacott - Economica (1999)\nR pour la Statistique et la science des données - Cornillon et al. - PUR (2018)"
  },
  {
    "objectID": "statistiques_descriptives.html",
    "href": "statistiques_descriptives.html",
    "title": "Statistiques descriptives",
    "section": "",
    "text": "Type de variable\nMode de représentation\nIndicateurs numériques\n\n\n\n\nQualitatives\nDiagramme en barres ou circulaire\nEffectifs en fonction des modalités\n\n\nQuantitatives discrètes\nDiagrammes en bâtons ou boîte à moustaches\nIndicateurs de tendance centrale :\n\nMoyenne\nMédiane\nMode\n\nIndicateurs de dispersion :\n\nVariance\nEcart-type (même unité que les données initiales)\n\n\n\nQuantitatives continues\nHistogramme ou boîte à moustaches\nIdem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType de combinaison\nMode de représentation\nComparaison numérique\n\n\n\n\nQualitative x Quantitative\nComparaison de boîtes à moustaches\n\nDécomposition de variance\nRapport de corrélation\n\n\n\nQuantitative x Quantitative\nNuage de points\nCovariance et corrélation (moyenne des produits termes à termes des données centrées-réduites)\n\n\nQualitative x Qualitative\nDiagrammes en barres pour chaque modalité\nTest du chi² (écart à l’indépendance)"
  },
  {
    "objectID": "statistiques_descriptives.html#eléments-de-vocabulaire",
    "href": "statistiques_descriptives.html#eléments-de-vocabulaire",
    "title": "Statistiques descriptives",
    "section": "Eléments de vocabulaire",
    "text": "Eléments de vocabulaire\n\nStatistique inférentielle = estimer des informations sur une population à partir d’un échantillon.\nStatistique descriptive = décrire les données. C’est un préalable à toute étude statistique"
  },
  {
    "objectID": "statistiques_descriptives.html#statistiques-descriptives-univariées",
    "href": "statistiques_descriptives.html#statistiques-descriptives-univariées",
    "title": "Statistiques descriptives",
    "section": "Statistiques descriptives univariées",
    "text": "Statistiques descriptives univariées\n\nVariables qualitatives\n\nModalités de représentation graphique\nLes modalités de représentation sur les diagrammes en barres.\n\nplot(iris$Species, col = \"#d8af90\")  # Alternative : barplot(iris$Species, col = \"#d8af90\")}\n\n\n\n\n\n# Dans ggplot\nggplot(data = iris, mapping = aes(x = Species)) +\n  geom_bar(fill = \"lightgrey\", col = \"darkgrey\") + \n  theme_classic() +\n  labs(title = \"Nombre d'iris en fonction des espèces\", \n       y = \"Individus\", \n       x = \"Espèces\")\n\n\n\n\nLes données peuvent également être représentées via un diagramme circulaire.\n\npie(table(iris$Species), col = c(\"#d8af90\", \"#aac1ce\", \"#bed6d2\"))\n\n\n\n\n\n# Dans ggplot\nTable_Especes &lt;- as.data.frame(table(iris$Species))\n\nggplot(data = Table_Especes, mapping = aes(x=\"\", y = Freq, fill = Var1)) +\n  geom_bar(stat=\"identity\", width=1) + \n  coord_polar(\"y\", start=0)\n\n\n\n\n\n\n\nVariables quantitatives\n\nModalités de représentation\n\nPour les variables discrètes : diagrammes en bâtons\n\n\nSL_discr &lt;- round(iris$Sepal.Length, 0)\nbarplot(table(SL_discr))\n\n\n\n\n\nPour les variables continues : histogrammes\n\n\nhist(iris$Sepal.Length)\n\n\n\n\n\nLes boites à moustaches permettent de montrer les quartiles\n\n\n\nIndicateurs numériques\n\nIndicateurs de tendance centrale\nLa moyenne est le centre de gravité des observations.\n\nmean(iris$Sepal.Length)\n\n[1] 5.843333\n\n\nLa médiane est le centre de distribution des valeurs.\n\nmedian(iris$Sepal.Length)\n\n[1] 5.8\n\n\nLes quartiles séparent une série en sous-groupes d’effectifs similaires.\n\nSL_Quarti &lt;- cut(iris$Sepal.Length, \n                 breaks = quantile(x = iris$Sepal.Length, \n                                   probs = seq(0, 1, 0.25)), \n                 include.lowest = TRUE) # l'argument probs permet de modifier les quantiles sélectionnés \ntable(SL_Quarti)\n\nSL_Quarti\n[4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] \n       41        39        35        35 \n\n\n\n\nIndicateurs de dispersion\nLa variance est la moyenne des carrés des écarts à la moyenne.\n\nvar(iris$Sepal.Length)\n\n[1] 0.6856935\n\n\nL’écart-type (noté sigma minuscule) est la racine carré de la variance. Il s’exprime donc dans la même unité que les données initiales.\n\nsd(iris$Sepal.Length)\n\n[1] 0.8280661\n\n\n\nCentrer les données consiste à retrancher la moyenne aux valeurs de la série. Le signe indique où se situe la valeur par rapport à la moyenne (si la valeur centrée est positive, la “vraie” valeur est supérieure à la moyenne). Les données centrées ont une moyenne nulle.\nRéduire les données consiste les diviser par l’écart-type. Les données réduites sont sans unité.\n\nLes données centrées-réduites ont comme propriétés d’être de moyenne nulle et d’écart-type égal à 1.\n\nhead(scale(iris$Sepal.Length), 5) # la fonction scale permet de centrer-réduire les données\n\n           [,1]\n[1,] -0.8976739\n[2,] -1.1392005\n[3,] -1.3807271\n[4,] -1.5014904\n[5,] -1.0184372\n\n\nCentrer-réduire permet de situer un individu plus facilement par rapport à la série. S’il se situe à plus de 2 écarts-types de la moyenne, il en est “vraiment” très loin.\nPour aller plus loin :\n\nInégalité de Markov\nInégalité de Bienaymé-Tchebychev"
  },
  {
    "objectID": "statistiques_descriptives.html#statistiques-descriptives-bivariées",
    "href": "statistiques_descriptives.html#statistiques-descriptives-bivariées",
    "title": "Statistiques descriptives",
    "section": "Statistiques descriptives bivariées",
    "text": "Statistiques descriptives bivariées\n\nQualitatif x Quantitatif\n\nReprésentation graphique\nLa représentation graphique peut être réalisée via la comparaison des boites à moustaches. On étudie alors la distribution conditionnelle.\n\nplot(iris$Sepal.Length ~ iris$Species)\n\n\n\n\nPour voir les résultats sous forme de tableau :\n\nAggr &lt;- aggregate(iris$Sepal.Length, by = list(iris$Species), FUN = quantile) \ntibble(Aggr)\n\n# A tibble: 3 × 2\n  Group.1    x[,\"0%\"] [,\"25%\"] [,\"50%\"] [,\"75%\"] [,\"100%\"]\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa          4.3     4.8       5        5.2       5.8\n2 versicolor      4.9     5.6       5.9      6.3       7  \n3 virginica       4.9     6.22      6.5      6.9       7.9\n\n\nDans le tidyverse, les modalités de rédaction sont les suivantes :\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  reframe(Quantile = quantile(Sepal.Length))\n\n# A tibble: 15 × 2\n   Species    Quantile\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 setosa         4.3 \n 2 setosa         4.8 \n 3 setosa         5   \n 4 setosa         5.2 \n 5 setosa         5.8 \n 6 versicolor     4.9 \n 7 versicolor     5.6 \n 8 versicolor     5.9 \n 9 versicolor     6.3 \n10 versicolor     7   \n11 virginica      4.9 \n12 virginica      6.22\n13 virginica      6.5 \n14 virginica      6.9 \n15 virginica      7.9 \n\n\n\n\nDécomposition de variance\nLa quantification du lien entre les variables peut être réalisée par la décomposition de la variance. L’équation de décomposition de la variance sépare la variabilité totale de Y en deux termes dont l’un peut être attribué au facteur X.\nIl s’agit de l’étude de la variance entre deux groupes :\n\nLa variance intra-groupe : moyenne des variances propres à chaque modalité (ici : la moyenne des variances des longueurs de pétales en fonction de l’espèces)\nLa variance inter-groupe : écarts des moyennes de groupe (ici : la variance des longueurs de pétales à laquelle on retire la variance intra).\n\nLa variance inter-groupes est la part qu’on attribue à X :\n\nSi elle est nulle : les moyennes de groupe sont égales à la moyenne générale donc égales entre elles : il n’y a pas de différence en moyenne entre les groupes du fait de l’action du facteur\nSi la variance inter-groupe est grande : une ou plusieurs moyennes de groupes diffèrent singulièrement de la moyenne générale. Si la variance inter-groupe est nulle : on n’observe qu’une seule et même valeur par groupe et le facteur explique donc toute la variabilité de Y.\n\nLe rapport de corrélation est la proportion dans la variabilité totale de Y représentée par la part attribuée au facteur X. Le rapport est compris entre 0 et 1 (η²).\nLe rapport de corrélation est obtenu de la façon suivante dans R :\n\nRes_ANOVA &lt;- anova(lm(Petal.Length~Species,data=iris)) \nRes_ANOVA[1,2]/sum(Res_ANOVA[,2])\n\n[1] 0.9413717\n\n\n\n\n\nQuantitatif x Quantitatif\n\nReprésentation graphique\nLa représentation graphique est réalisée via un nuage de points.\n\nplot(Sepal.Length ~ Petal.Length, data = iris)\n\n\n\n\n\n\nCovariance / Corrélation\nLa covariance donne une indication sur la manière dont les deux variables “covarient”.\n\ncov(x = iris$Sepal.Length, y = iris$Sepal.Width)\n\n[1] -0.042434\n\n\nLe coefficient de corrélationn linéaire est la moyenne des produits terme à terme des données centrées-réduites.\n\ncor(x = iris$Sepal.Length, y = iris$Sepal.Width)\n\n[1] -0.1175698\n\n\nIl est compris en -1 et 1, sans unité. S’il est égal à 1, cela correspond à des points alignés sur une droite de pente positive.\nLe coefficient de corrélation mesure l’intensité de la linéarité de la relation entre les observations. Une corrélation nulle ne garantit pas l’absence de tout lien mais indique l’absence de relation linéaire.\n\n\nModèle linéaire simple : droite des moindres carrés\nLa fonction lm() permet d’obtenir la droite de régression.\nLe coefficient de détermination R² est le rapport entre la variance des valeurs ajustées et la variance des valeurs de Y. C’est un rapport compris entre 0 et 1. Il s’agit de la variance des valeurs ajustées (fitted values)  divisée par la variance des valeurs observées.\n\n\n\nQualitatif x Qualitatif\n\nTableau de contingence\nOn résume les données dans le tableau de contingence, tableau qui regroupe les effectifs observés dans chaque croisement de modalités.\nEn appliquant la somme par ligne et par colonne on obtient les distributions de X et Y séparément. Les effectifs correspondant sont ceux des distributions marginales (univariées) des variables.\n\niris$Taille &lt;- as.factor(ifelse(iris$Sepal.Length &gt; mean(iris$Sepal.Length), \"Grande sépale\", \"Petite sépale\")) \ntab_contingence &lt;- table(iris$Species, iris$Taille) \napply(tab_contingence, FUN=sum, MARGIN=1) # ou MARGIN = 2\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nOn peut représenter les diagrammes en barres associés à chacune d’elles.\n\n\nDistributions conditionnelles\n\nround(100 * prop.table(tab_contingence, margin = 2), 1) # ou margin = 1 pour la distribution selon les lignes\n\n            \n             Grande sépale Petite sépale\n  setosa               0.0          62.5\n  versicolor          37.1          30.0\n  virginica           62.9           7.5\n\n\nOn peut représenter les tuyaux d’orgues associés.\n\nplot(round(100 * prop.table(tab_contingence, margin = 2), 1))\n\n\n\n\n\n\nChi² : mesurer le lien entre les variables\nOn quantifie le lien entre les variables en mesurant l’écart aux données avec une situation conceptuelle d’absence de lien que l’on appelle hypothèse d’indépendance des données :\n\nH0 est l’hypothèse d’indépendance\nH1 est l’hypothèse de dépendance\n\n\nchisq.test(tab_contingence)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab_contingence\nX-squared = 78.643, df = 2, p-value &lt; 2.2e-16\n\n\np-value : probabilité d’obtenir ce résultat si le jeu de données était indépendant. On prend le plus souvent la limite de 5% pour rejeter l’hypothèse d’indépendance. Dans l’exemple ci-dessus, on rejette l’hypothèse de l’indépendance entre le fait d’avoir une grande ou une petite sépale et l’espèce d’iris."
  },
  {
    "objectID": "données_manquantes.html",
    "href": "données_manquantes.html",
    "title": "Gestion des données manquantes",
    "section": "",
    "text": "Le % de données manquantes n’est pas le plus important. Il est nécessaire de comprendre les relations entre les variables. L’imputation d’un jeu de données ayant des variables fortement corrélées et un grand nombre de données manquantes sera de bonne qualité, à l’inverse de l’imputation d’un jeu de données avec peu de données manquante mais beaucoup de bruit.\nL’imputation simple ne tient pas compte de la variabilité induite par les données manquantes\nL’imputation multiple permet de tenir compte de cette variabilité et permet de mesurer précisément la variabilité des prédictions."
  },
  {
    "objectID": "données_manquantes.html#synthèse",
    "href": "données_manquantes.html#synthèse",
    "title": "Gestion des données manquantes",
    "section": "",
    "text": "Le % de données manquantes n’est pas le plus important. Il est nécessaire de comprendre les relations entre les variables. L’imputation d’un jeu de données ayant des variables fortement corrélées et un grand nombre de données manquantes sera de bonne qualité, à l’inverse de l’imputation d’un jeu de données avec peu de données manquante mais beaucoup de bruit.\nL’imputation simple ne tient pas compte de la variabilité induite par les données manquantes\nL’imputation multiple permet de tenir compte de cette variabilité et permet de mesurer précisément la variabilité des prédictions."
  },
  {
    "objectID": "données_manquantes.html#identification-des-valeurs-manquantes",
    "href": "données_manquantes.html#identification-des-valeurs-manquantes",
    "title": "Gestion des données manquantes",
    "section": "Identification des valeurs manquantes",
    "text": "Identification des valeurs manquantes\n\ndata(iris)\niris[seq(from = 1, to = nrow(iris), by = 10), 2] &lt;- NA\n\nLes fonctions de base :\n\nisna &lt;- is.na(iris) # Donne un vecteur de booléens\nanyNA(iris) # Oui / Non au moins une valeur manquante est présente\n\n[1] TRUE\n\nsummary(iris) # Montre les valeurs manquantes\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.200   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.055   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n                 NA's   :15                                     \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n                \n\n\nCes fonctions permettent de manipuler facilement les données manquantes :\n\niris2 &lt;- iris[-which(is.na(iris$Sepal.Width)),]\nanyNA(iris2)\n\n[1] FALSE\n\ndim(iris2)\n\n[1] 135   5\n\n\nPour supprimer l’ensemble des valeurs manquantes, on peut aussi utiliser la fonction na.omit.\n\niris3 &lt;- na.omit(iris)\ndim(iris3)\n\n[1] 135   5\n\n\nDans le tidyverse, d’autres fonctions sont disponibles :\n\niris4 &lt;- iris %&gt;% \n  drop_na()\ndim(iris4)\n\n[1] 135   5\n\n\n\niris5 &lt;- iris %&gt;% \n  fill(Sepal.Width, .direction = \"up\") # remplace les valeurs manquantes avec la valeur du dessous\nanyNA(iris5)\n\n[1] FALSE\n\n\n\niris6 &lt;- iris %&gt;%\n  replace_na(list(Sepal.Width = 10)) # attention, il faut une liste de valeurs"
  },
  {
    "objectID": "données_manquantes.html#visualisation-des-données-manquantes",
    "href": "données_manquantes.html#visualisation-des-données-manquantes",
    "title": "Gestion des données manquantes",
    "section": "Visualisation des données manquantes",
    "text": "Visualisation des données manquantes\nL’objectif est de répondre aux questions suivantes :\n\nQuelle est la proportion de données manquantes ?\nQuel est le type de données manquantes ?\nExiste-t-il une dépendance dans les données manquantes ?\n\nLes données manquantes relèvent de plusieurs types :\n\nMCAR - Missing completely at random. La probabilité qu’une observation soit manquante ne dépend pas des mesures observées ou non observées.\nMAR - Missing at random. La probabilité qu’une observation soit manquante dépend uniquement des données observées (ex : dans un questionnaire, la probabilité que la donnée sur le poids soit manquante dépend du sexe).\nMNAR - Mission not at random. La probabilité qu’une observation soit manquante dépend des données non observées et n’est pas aléatoire (ex : les personnes en sur-poids ne donnent pas leurs poids : les valeurs extrêmes sont absentes).\n\nLes packages naniar et VIM aident à visualiser les données manquantes.\n\niris_NA &lt;- iris\niris_NA[seq(from = 1, to = nrow(iris), by = 3), 4] &lt;- NA\niris_NA[seq(from = 1, to = nrow(iris), by = 6), 3] &lt;- NA\niris_NA[seq(from = 1, to = nrow(iris), by = 25), 5] &lt;- NA\nsummary(iris_NA)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.200   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.300   Median :1.300  \n Mean   :5.843   Mean   :3.055   Mean   :3.774   Mean   :1.209  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n                 NA's   :15      NA's   :25      NA's   :50     \n       Species  \n setosa    :48  \n versicolor:48  \n virginica :48  \n NA's      : 6  \n                \n                \n                \n\n\nLe package naniar facilite l’identification des valeurs manquantes :\n\nnaniar::pct_miss(iris_NA) # pct de données manquantes\n\n[1] 12.8\n\n\n\nnaniar::n_miss(iris_NA) # nb de données manquantes\n\n[1] 96\n\n\n\nnaniar::n_complete(iris_NA) # nb de données complètes\n\n[1] 654\n\n\n\nnaniar::pct_complete(iris_NA) # pct de données complètes\n\n[1] 87.2\n\n\n\nInformations sur les variables\n\nnaniar::miss_var_summary(iris_NA) # détail des informations en fonction des variables\n\n# A tibble: 5 × 3\n  variable     n_miss pct_miss\n  &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;\n1 Petal.Width      50     33.3\n2 Petal.Length     25     16.7\n3 Sepal.Width      15     10  \n4 Species           6      4  \n5 Sepal.Length      0      0  \n\n\n\nnaniar::gg_miss_var(iris_NA, show_pct = TRUE) + ylim(0, 100)\n\n\n\n\n\n\nInformations sur les individus\n\nnaniar::miss_case_summary(iris_NA)\n\n# A tibble: 150 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     1      4       80\n 2    31      3       60\n 3    61      3       60\n 4    91      3       60\n 5   121      3       60\n 6     7      2       40\n 7    13      2       40\n 8    19      2       40\n 9    25      2       40\n10    37      2       40\n# ℹ 140 more rows\n\n\n\nnaniar::gg_miss_case(iris_NA)\n\n\n\n\n\nnaniar::prop_miss_row(iris_NA)\n\n  [1] 0.8 0.0 0.0 0.2 0.0 0.0 0.4 0.0 0.0 0.2 0.2 0.0 0.4 0.0 0.0 0.2 0.0 0.0\n [19] 0.4 0.0 0.2 0.2 0.0 0.0 0.4 0.2 0.0 0.2 0.0 0.0 0.6 0.0 0.0 0.2 0.0 0.0\n [37] 0.4 0.0 0.0 0.2 0.2 0.0 0.4 0.0 0.0 0.2 0.0 0.0 0.4 0.0 0.4 0.2 0.0 0.0\n [55] 0.4 0.0 0.0 0.2 0.0 0.0 0.6 0.0 0.0 0.2 0.0 0.0 0.4 0.0 0.0 0.2 0.2 0.0\n [73] 0.4 0.0 0.0 0.4 0.0 0.0 0.4 0.0 0.2 0.2 0.0 0.0 0.4 0.0 0.0 0.2 0.0 0.0\n [91] 0.6 0.0 0.0 0.2 0.0 0.0 0.4 0.0 0.0 0.2 0.4 0.0 0.4 0.0 0.0 0.2 0.0 0.0\n[109] 0.4 0.0 0.2 0.2 0.0 0.0 0.4 0.0 0.0 0.2 0.0 0.0 0.6 0.0 0.0 0.2 0.0 0.2\n[127] 0.4 0.0 0.0 0.2 0.2 0.0 0.4 0.0 0.0 0.2 0.0 0.0 0.4 0.0 0.2 0.2 0.0 0.0\n[145] 0.4 0.0 0.0 0.2 0.0 0.0\n\n\n\nnaniar::vis_miss(iris_NA)\n\n\n\n\n\nnaniar::vis_miss(iris_NA, cluster = TRUE) # On peut regrouper les données pour essayer de trouver les relations entre les données manquantes\n\n\n\n\nLe package VIM offre d’autres solutions de visualisation :\n\nVIM::aggr(iris_NA, sortVAR = TRUE)\n\n\n\n\n\niris_NA %&gt;% \n  select(Sepal.Length, Petal.Width) %&gt;% \n  VIM::marginplot()\n\n\n\n\nLa visualisation et l’analyse des données manquantes peut aussi être réalisée en mobilisant une ACM :\n\ndata_miss &lt;- data.frame(is.na(iris_NA))\ndata_miss &lt;- apply(X=data_miss, FUN=function(x) if(x) \"m\" else \"o\", MARGIN=c(1,2))\nres.mca &lt;- FactoMineR::MCA(data_miss, graph = F)\nplot(res.mca, invis = \"ind\", title = \"graphe des modalités\", cex  = 0.5)"
  },
  {
    "objectID": "données_manquantes.html#imputer-les-valeurs-manquantes",
    "href": "données_manquantes.html#imputer-les-valeurs-manquantes",
    "title": "Gestion des données manquantes",
    "section": "Imputer les valeurs manquantes",
    "text": "Imputer les valeurs manquantes\nLa suppression des données manquantes est envisageable si le nombre d’observations complètes est suffisant (reste à définir un seuil acceptable…).\nL’imputation consiste à compléter le jeu de données :\n\nImputation simple : prévoir les valeurs manquantes pour une analyse exploratoire.\nImputation multiple : faire de l’inférence.\n\n\nImputation simple\nL’imputation peut être réalisée en remplaçant les valeurs manquantes par la moyenne des valeurs observées de la variable.\n\niris_NA %&gt;% \n  mutate(new_Sepal.Width = replace_na(Sepal.Width, mean(Sepal.Width, na.rm = TRUE))) %&gt;% \n  select(Sepal.Width, new_Sepal.Width) %&gt;% \n  summary()\n\n  Sepal.Width    new_Sepal.Width\n Min.   :2.200   Min.   :2.200  \n 1st Qu.:2.800   1st Qu.:2.800  \n Median :3.000   Median :3.000  \n Mean   :3.055   Mean   :3.055  \n 3rd Qu.:3.300   3rd Qu.:3.275  \n Max.   :4.400   Max.   :4.400  \n NA's   :15                     \n\n\nCette méthode entraîne néanmoins une déformation de la distribution des valeurs., Une autre méthode consiste à imputer en réalisant une ACP :\n\nnb &lt;- missMDA::estim_ncpPCA(iris_NA[,1:4],ncp.max=5) # estimation du nb de composantes\ndat.comp &lt;- missMDA::imputePCA(iris_NA[,1:4],ncp=nb$ncp) # imputation du jeu de données\nhead(dat.comp$completeObs)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          5.1    3.109677      2.30904   0.6069165\n[2,]          4.9    3.000000      1.40000   0.2000000\n[3,]          4.7    3.200000      1.30000   0.2000000\n[4,]          4.6    3.100000      1.50000   0.2514280\n[5,]          5.0    3.600000      1.40000   0.2000000\n[6,]          5.4    3.900000      1.70000   0.4000000\n\n\nLa fonction imputeMCA permet de faire l’imputation avec des données qualitatives et imputeFAMD avec des données mixtes.\n\nnb2 &lt;- missMDA::estim_ncpFAMD(iris_NA, ncp.max=5, verbose = FALSE) # estimation du nb de composantes\ndat.comp2 &lt;- missMDA::imputeFAMD(iris_NA,ncp=nb2$ncp) # imputation du jeu de données\nhead(dat.comp2$completeObs)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1    3.091835     2.382436   0.6625403  setosa\n2          4.9    3.000000     1.400000   0.2000000  setosa\n3          4.7    3.200000     1.300000   0.2000000  setosa\n4          4.6    3.100000     1.500000   0.2041850  setosa\n5          5.0    3.600000     1.400000   0.2000000  setosa\n6          5.4    3.900000     1.700000   0.4000000  setosa\n\nsummary(dat.comp2$completeObs)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    \n Min.   :4.300   Min.   :2.200   Min.   :1.000   Min.   :0.1000  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.3284  \n Median :5.800   Median :3.000   Median :4.200   Median :1.3000  \n Mean   :5.843   Mean   :3.060   Mean   :3.747   Mean   :1.1969  \n 3rd Qu.:6.400   3rd Qu.:3.356   3rd Qu.:5.100   3rd Qu.:1.8590  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.5000  \n       Species  \n setosa    :50  \n versicolor:49  \n virginica :51  \n                \n                \n                \n\n\nL’imputation peut également être réalisée par imputation aléatoire :\n\niris_missforest &lt;- missForest::missForest(iris_NA)\nsummary(iris_missforest$ximp)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.200   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.200   Median :1.300  \n Mean   :5.843   Mean   :3.052   Mean   :3.749   Mean   :1.198  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :49  \n versicolor:51  \n virginica :50"
  },
  {
    "objectID": "données_manquantes.html#imputation-multiple",
    "href": "données_manquantes.html#imputation-multiple",
    "title": "Gestion des données manquantes",
    "section": "Imputation multiple",
    "text": "Imputation multiple\nL’objectif est de générer plusieurs tableaux de données imputés pour réfleter la variance de prédiction des valeurs manquantes.\n\nres.mipca &lt;- missMDA::MIPCA(iris_NA[1:4], scale = TRUE, ncp = nb$ncp, nboot = 100)\nplot(res.mipca)\n\n\n\n\n\n\n\n\n\n\n\n\n\n$PlotIndProc\n\n\n\n\n\n\n$PlotDim\n\n\n\n\n\n\n$PlotIndSupp\n\n\n\n\n\n\n$PlotVar\n\n\n\n\n\nLe schéma bleu permet de voir si le % de données manquantes impacte la qualité de l’imputation.\nLa taille des ellipses et la dispersion des points des variables permettent de savoir si on peut poursuivre l’analyse des tableaux imputés.\nOn peut utiliser l’imputation mutlple avec hypothèse de distribution jointe gausienne :\n\ndat.amelia &lt;- Amelia::amelia(iris_NA[1:4], m=100, p2s = 0) \nAmelia::compare.density(dat.amelia,var=\"Petal.Length\")\n\n\n\n\nOn peut aussi utiliser la méthode d’imputation multiple selon le modèle conditionnel :\n\nmice_mice &lt;- mice::mice(iris_NA, m=5, method=\"pmm\", printFlag = FALSE)\nhead(mice::complete(mice_mice,1), 5) # pour visualiser le 1er taleau généré\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         4.1          1.3         0.1  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n\nmice_mice$meth # permet de visualiser les méthodes utilisées\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          \"\"        \"pmm\"        \"pmm\"        \"pmm\"        \"pmm\""
  },
  {
    "objectID": "classification_NS.html",
    "href": "classification_NS.html",
    "title": "Classification non supervisée",
    "section": "",
    "text": "Les démarches de classification visent à décrire des ensembles de grande dimension. La classification non-supervisée consiste à répartir en classes, en catégories, des objets ayant des caractères communs afin notamment d’en faciliter l’étude.\nEn fonction du nombre d’individus, le nombre de combinaisons possibles devient vite grand. Il faut donc une solution pour rechercher les partitions optimales. Différentes méthodes peuvent être mises en oeuvre :\n\nLe partitionnement non-hiérarchique (k-means)\nLe partitionnement hiérarchique qui regroupe (méthode ascendante) puis divise (méthode descendante) les individus de manière séquentielle (CAH et CDH)\nUne méthode basée sur la densité des points (DBSCAN)\nLes méthodes probabilistes basées sur des modèles de mélange de lois (EM, SEM, …)\n\nQuelle que soit la méthode, il faut définir :\n\nUne mesure de dissimilarité ou similarité entre les individus. Il s’agit ainsi de faire en sorte que les individus soient les plus similaires possibles dans un groupe (variabilité intra-classe faible).\nUne mesure de l’homogénéité des groupes et la différence entre les différents groupes. Il s’agit ainsi de faire en sorte que les groupes soient les plus différents entre eux (variabilité inter-classe grande).\n\nInertie (notion de variabilité) : distance au carré de tous les points à la moyenne du groupe. Dans le cadre d’une démarche de classification, l’inertie totale est égale à la somme entre la variabilité inter-classe et de la variabilité intra-classe, donc minimiser un des paramètres revient à maximiser l’autre. Le ratio inertie inter / inertie totale est compris entre 0 et 1, plus il est proche de 1, plus les individus au sein des classes sont homogènes et les classes différentes.\nAttention : avant de réaliser toute classification, il faut centrer-réduire les données. Les données utilisées ne peuvent, en outre, n’être que quantitatives. Si des variables qualitatives doivent être incluses, nous pouvons récupérer leurs coordonnées sur les principaux plans factoriels après avoir réalisé une AFC."
  },
  {
    "objectID": "classification_NS.html#objectifs",
    "href": "classification_NS.html#objectifs",
    "title": "Classification non supervisée",
    "section": "",
    "text": "Les démarches de classification visent à décrire des ensembles de grande dimension. La classification non-supervisée consiste à répartir en classes, en catégories, des objets ayant des caractères communs afin notamment d’en faciliter l’étude.\nEn fonction du nombre d’individus, le nombre de combinaisons possibles devient vite grand. Il faut donc une solution pour rechercher les partitions optimales. Différentes méthodes peuvent être mises en oeuvre :\n\nLe partitionnement non-hiérarchique (k-means)\nLe partitionnement hiérarchique qui regroupe (méthode ascendante) puis divise (méthode descendante) les individus de manière séquentielle (CAH et CDH)\nUne méthode basée sur la densité des points (DBSCAN)\nLes méthodes probabilistes basées sur des modèles de mélange de lois (EM, SEM, …)\n\nQuelle que soit la méthode, il faut définir :\n\nUne mesure de dissimilarité ou similarité entre les individus. Il s’agit ainsi de faire en sorte que les individus soient les plus similaires possibles dans un groupe (variabilité intra-classe faible).\nUne mesure de l’homogénéité des groupes et la différence entre les différents groupes. Il s’agit ainsi de faire en sorte que les groupes soient les plus différents entre eux (variabilité inter-classe grande).\n\nInertie (notion de variabilité) : distance au carré de tous les points à la moyenne du groupe. Dans le cadre d’une démarche de classification, l’inertie totale est égale à la somme entre la variabilité inter-classe et de la variabilité intra-classe, donc minimiser un des paramètres revient à maximiser l’autre. Le ratio inertie inter / inertie totale est compris entre 0 et 1, plus il est proche de 1, plus les individus au sein des classes sont homogènes et les classes différentes.\nAttention : avant de réaliser toute classification, il faut centrer-réduire les données. Les données utilisées ne peuvent, en outre, n’être que quantitatives. Si des variables qualitatives doivent être incluses, nous pouvons récupérer leurs coordonnées sur les principaux plans factoriels après avoir réalisé une AFC."
  },
  {
    "objectID": "classification_NS.html#k-means",
    "href": "classification_NS.html#k-means",
    "title": "Classification non supervisée",
    "section": "K-means",
    "text": "K-means\n\nPrésentation générale\nLe sujet d’une classification est ainsi, à K fixé (c’est-à-dire le nombre de groupes), de minimiser l’inertie intra-classes, donc rendre les classes les plus homogènes possibles, et de maximiser l’inertie inter-classes, donc séparer le plus possible les classes entre elles. La qualité d’un clustering peut être évaluée par le rapport : Inertieinter / inertietotale. C’est l’objectif visé par la méthode des k-means.\nPour appliquer la méthode des k-means, il faut choisir :\n\nLe nombre de groupes K. Il peut être fait :\n\nEn fonction d’une connaissance a priori de la base\nA la suite d’une CAH\nSelon un critère ad-hoc : « coude » dans la représentation graphique de l’inertie intra-classes.\n\n\nPour ce faire, on peut utiliser la boucle suivante :\n\n# Création des données et centrage réduction\n\ndf &lt;- data.frame(Taille = c(168, 158, 177, 193, 178, 160, 180, 175, 189, 182), \n                 Pointure = c(41, 37, 42, 45, 41, 37, 42, 39, 44, 45))\ndfcr &lt;- as.data.frame(scale(df))\n\n# Boucle définition du nombre de K\n\nnbg &lt;- 1:9 \n\ndifintra &lt;-  1:9 \n\nfor (ii in 1:max(nbg)) {   \n  tmp &lt;- kmeans(dfcr, centers = ii)   \n  difintra[ii] &lt;- tmp$betweens \n} \n\nplot(nbg, difintra/tmp$totss*100, type = \"h\")\n\n\n\n\n\nLa distance entre vecteurs (qui est la distance euclidienne)\nLe représentant de chaque groupe (moyenne du groupe)\nLe point de départ de l’algorithme\n\nDans RStudio, mettre en œuvre la méthode se fait de la façon suivante :\n\ndata(iris) \niris_cr &lt;- scale(iris[1:4]) \nkm &lt;- kmeans(x = iris_cr, centers = 3) \niris$km &lt;- km$cluster  \niris %&gt;%    \n  ggplot(aes(Sepal.Length, Petal.Width, col = km)) +\n  geom_point() +   \n  theme_bw()\n\n\n\n\n\n\nModalités de mise en oeuvre\n\nImport et sélection des données à utiliser. Les variables doivent être quantitatives. Si les variables sont qualitatives, on peut récupérer les coordonnées à la suite d’une analyse factorielle.\nCentrer-réduire les données\nConstruire la partition (via la fonction kmeans)\nCaractériser les classes. Construire des classes n’a de sens que si on est capables de comprendre les logiques qui ont poussé au rapprochement entre les individus. Plusieurs indices peuvent être utilisés :\n\n\nL’étude du parangon (individu moyen, individu le plus proche du centre de la classe) de chaque variable)\nEtudier les variables qui caractérisent le mieux la partition. On peut considérer la partition comme une variable qualitative (avec autant de modalités que de classes). Pour chaque variable quantitative, on construit un modèle d’analyse de variance et on trie les variables par probabilités critiques croissantes.\n\n\nlibrary(FactoMineR)\niris$km &lt;- as.factor(iris$km) \ncatdes(iris, num.var = 6)\n\n\nLink between the cluster variable and the categorical variables (chi-square test)\n=================================================================================\n             p.value df\nSpecies 1.707502e-39  4\n\nDescription of each cluster by the categories\n=============================================\n$`1`\n                   Cla/Mod  Mod/Cla   Global      p.value    v.test\nSpecies=versicolor      78 73.58491 33.33333 1.297000e-14  7.706121\nSpecies=setosa           0  0.00000 33.33333 6.075543e-13 -7.198776\n\n$`2`\n                   Cla/Mod Mod/Cla   Global      p.value    v.test\nSpecies=setosa         100     100 33.33333 4.968040e-41 13.414570\nSpecies=virginica        0       0 33.33333 5.012323e-12 -6.905227\nSpecies=versicolor       0       0 33.33333 5.012323e-12 -6.905227\n\n$`3`\n                  Cla/Mod  Mod/Cla   Global      p.value    v.test\nSpecies=virginica      72 76.59574 33.33333 6.526248e-14  7.497065\nSpecies=setosa          0  0.00000 33.33333 3.783976e-11 -6.612299\n\n\nLink between the cluster variable and the quantitative variables\n================================================================\n                  Eta2      P-value\nPetal.Length 0.9213177 7.029961e-82\nPetal.Width  0.8773362 1.048715e-67\nSepal.Length 0.7483489 9.093246e-45\nSepal.Width  0.5208606 3.265505e-24\n\nDescription of each cluster by quantitative variables\n=====================================================\n$`1`\n                v.test Mean in category Overall mean sd in category Overall sd\nPetal.Length  3.137597         4.369811     3.758000      0.5551083  1.7594041\nPetal.Width   2.540187         1.413208     1.199333      0.3083968  0.7596926\nSepal.Width  -7.970600         2.673585     3.057333      0.2518992  0.4344110\n                  p.value\nPetal.Length 1.703391e-03\nPetal.Width  1.107932e-02\nSepal.Width  1.579055e-15\n\n$`2`\n                 v.test Mean in category Overall mean sd in category Overall sd\nSepal.Width    7.364799            3.428     3.057333      0.3752546  0.4344110\nSepal.Length  -8.757174            5.006     5.843333      0.3489470  0.8253013\nPetal.Width  -10.831410            0.246     1.199333      0.1043264  0.7596926\nPetal.Length -11.263787            1.462     3.758000      0.1719186  1.7594041\n                  p.value\nSepal.Width  1.774142e-13\nSepal.Length 2.002060e-18\nPetal.Width  2.443627e-27\nPetal.Length 1.980605e-29\n\n$`3`\n               v.test Mean in category Overall mean sd in category Overall sd\nSepal.Length 9.366782         6.780851     5.843333      0.4853769  0.8253013\nPetal.Width  8.390136         1.972340     1.199333      0.3272725  0.7596926\nPetal.Length 8.213914         5.510638     3.758000      0.6311873  1.7594041\n                  p.value\nSepal.Length 7.477757e-21\nPetal.Width  4.855788e-17\nPetal.Length 2.140928e-16\n\n\nLa sortie représente les variables les plus liées à la variable de classe. Une valeur-test supérieure à 2 en valeur absolue signifie que la moyenne de la classe est significativement différente de la moyenne générale. Un signe positif de la valeur-test indique que la moyenne de la classe est supérieure à la moyenne générale."
  },
  {
    "objectID": "classification_NS.html#classification-ascendante-hiérarchique-cah",
    "href": "classification_NS.html#classification-ascendante-hiérarchique-cah",
    "title": "Classification non supervisée",
    "section": "Classification ascendante hiérarchique (CAH)",
    "text": "Classification ascendante hiérarchique (CAH)\n\nPrésentation générale\nLe dendrogramme représente, sous forme d’arbre binaire, les agrégations successives jusqu’à la réunion en une seule classe de tous les individus. On parle de racine (1 seule classe), de feuilles (n classes), de branches et de nœuds. Elle met en avant les liens hiérarchiques entre les individus.\nLa hauteur d’une branche est égale à l’indice de la hiérarchie, soit usuellement la distance (ultramétrique) entre les deux sous-groupes regroupés. La hauteur donne la difficulté pour deux groupes d’individus à être réunis dans le même groupe.\nLorsqu’on coupe l’arbre, on peut comptabiliser le nombre de classes retenues.\nEn coupant le dendrogramme au niveau d’un saut important, on espère obtenir une partition de bonne qualité : les individus regroupés auparavant étaient proches, tandis que ceux regroupés après la coupure deviennent trop éloignés.\nDans RStudio :\n\nhclust &lt;- hclust(d = dist(iris[,1:4]), method = \"single\")\nplot(as.dendrogram(hclust))\n\n\n\n\nPour visualiser le gain d’inertie :\n\nplot(hclust[[2]], type = \"h\")\n\n\n\n\nPour finaliser la classification :\n\ncutree(hclust, k = 2)\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\n\n\n\nModalités de mise en oeuvre\n\nImport et sélection des données à utiliser. Les variables doivent être quantitatives. Si les variables sont qualitatives, on peut récupérer les coordonnées à la suite d’une analyse factorielle.\nCentrer-réduire les données\nConstruire le dendrogramme (fonction hclust)\nDéfinir le nombre de classes à sélectionner. Pour ce faire, il faut visualiser le graphique d’analyse d’évolution de l’inertie inter-classe (deuxième élément de la liste produite par la fonction hclust)\nCaractériser les classes. Construire des classes n’a de sens que si on est capables de comprendre les logiques qui ont poussé au rapprochement entre les individus (comme pour les k-means)."
  },
  {
    "objectID": "statistiques_descriptives.html#synthèse",
    "href": "statistiques_descriptives.html#synthèse",
    "title": "Statistiques descriptives",
    "section": "",
    "text": "Type de variable\nMode de représentation\nIndicateurs numériques\n\n\n\n\nQualitatives\nDiagramme en barres ou circulaire\nEffectifs en fonction des modalités\n\n\nQuantitatives discrètes\nDiagrammes en bâtons ou boîte à moustaches\nIndicateurs de tendance centrale :\n\nMoyenne\nMédiane\nMode\n\nIndicateurs de dispersion :\n\nVariance\nEcart-type (même unité que les données initiales)\n\n\n\nQuantitatives continues\nHistogramme ou boîte à moustaches\nIdem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType de combinaison\nMode de représentation\nComparaison numérique\n\n\n\n\nQualitative x Quantitative\nComparaison de boîtes à moustaches\n\nDécomposition de variance\nRapport de corrélation\n\n\n\nQuantitative x Quantitative\nNuage de points\nCovariance et corrélation (moyenne des produits termes à termes des données centrées-réduites)\n\n\nQualitative x Qualitative\nDiagrammes en barres pour chaque modalité\nTest du chi² (écart à l’indépendance)"
  },
  {
    "objectID": "web_scraping.html",
    "href": "web_scraping.html",
    "title": "Web scraping",
    "section": "",
    "text": "Le web scraping nécessite d’utiliser le package rvest.\nlibrary(tidyverse)\nlibrary(rvest)"
  },
  {
    "objectID": "web_scraping.html#récupération-simple",
    "href": "web_scraping.html#récupération-simple",
    "title": "Web scraping",
    "section": "Récupération simple",
    "text": "Récupération simple\nPour obtenir un élément sur une page :\n\nurl &lt;- read_html(\"https://www.pokekalos.fr/pokedex/pokemongo/index.html\")\n\n\nnoms &lt;- url %&gt;% \n  html_nodes(\".pkgo-header\") %&gt;% \n  html_text()\nnoms[1:10]\n\n [1] \"Bulbizarre\"       \"Herbizarre\"       \"Florizarre\"       \"Méga-Florizarre\" \n [5] \"Salamèche\"        \"Reptincel\"        \"Dracaufeu\"        \"Méga-Dracaufeu X\"\n [9] \"Méga-Dracaufeu Y\" \"Carapuce\"        \n\n\nPour obtenir un tableau :\n\nwiki &lt;- read_html(\"https://fr.wikipedia.org/wiki/R%C3%A9gion_fran%C3%A7aise\")\n\n\ntableaux &lt;- wiki %&gt;%    \n  html_nodes(\"table\") %&gt;%   \n  html_table()\n\n\ntableau_region &lt;- tableaux[[6]] \nhead(tableau_region, 5)\n\n# A tibble: 5 × 10\n  Carte Dénomination               `Chef-lieu de région` `Départements[Note 2]` \n  &lt;chr&gt; &lt;chr&gt;                      &lt;chr&gt;                 &lt;chr&gt;                  \n1 \"\"    Auvergne-Rhône-Alpes[6]    Lyon[6]               12 (+ Métropole de Lyo…\n2 \"\"    Bourgogne-Franche-Comté[7] Dijon[7]              8  (Côte-d'Or, Doubs, …\n3 \"\"    Bretagne                   Rennes[8]             4  (Côtes-d'Armor, Fin…\n4 \"\"    Centre-Val de Loire        Orléans[8]            6  (Cher, Eure-et-Loir…\n5 \"\"    Corse[Note 3]              Ajaccio[8]            2  (Corse-du-Sud et Ha…\n# ℹ 6 more variables: `Superficie (km2)[2]` &lt;chr&gt;,\n#   `Population (2019)[3]` &lt;chr&gt;, `Population estimée (2023)[4]` &lt;chr&gt;,\n#   `Densité (2019) (hab./km2)` &lt;chr&gt;, `Code Insee[5]` &lt;int&gt;,\n#   `Président(e)` &lt;chr&gt;"
  },
  {
    "objectID": "web_scraping.html#méthode-de-crawling",
    "href": "web_scraping.html#méthode-de-crawling",
    "title": "Web scraping",
    "section": "Méthode de crawling",
    "text": "Méthode de crawling\nSi on veut récupérer des informations qui sont dans une page “complémentaire” :\n\npoke_links &lt;- url %&gt;% \n  html_nodes(\".pokemonlink\") %&gt;% \n  html_attr(\"href\") %&gt;% \n  paste(\"https://www.pokekalos.fr\", ., sep = \"\")\npoke_links[1:10]\n\n [1] \"https://www.pokekalos.fr/pokedex/pokemongo/bulbizarre-1.html\"        \n [2] \"https://www.pokekalos.fr/pokedex/pokemongo/herbizarre-2.html\"        \n [3] \"https://www.pokekalos.fr/pokedex/pokemongo/florizarre-3.html\"        \n [4] \"https://www.pokekalos.fr/pokedex/pokemongo/mega-florizarre-3m.html\"  \n [5] \"https://www.pokekalos.fr/pokedex/pokemongo/salameche-4.html\"         \n [6] \"https://www.pokekalos.fr/pokedex/pokemongo/reptincel-5.html\"         \n [7] \"https://www.pokekalos.fr/pokedex/pokemongo/dracaufeu-6.html\"         \n [8] \"https://www.pokekalos.fr/pokedex/pokemongo/mega-dracaufeu-x-6mx.html\"\n [9] \"https://www.pokekalos.fr/pokedex/pokemongo/mega-dracaufeu-y-6my.html\"\n[10] \"https://www.pokekalos.fr/pokedex/pokemongo/carapuce-7.html\"          \n\n\nAfin d’obtenir les informations contenues sur ces pages, on peut procéder de la sorte :\n\nget_data &lt;- function(links){\n  Sys.sleep(0.5)\n  page &lt;- read_html(links)\n  restmp &lt;- list()\n  restmp$numero = page %&gt;% html_nodes(\".message+ .bloc-principal-dex .bloc-dex:nth-child(1) .description\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$nom = page %&gt;% html_nodes(\".mb-0\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$femelle = page %&gt;% html_nodes(\".description div:nth-child(1)\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$PV = page %&gt;% html_nodes(\".column4:nth-child(1) span\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$ATK = page %&gt;% html_nodes(\".column4:nth-child(2) span\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$DEF = page %&gt;% html_nodes(\".column4~ .column4+ .column4 span\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$taille = page %&gt;% html_nodes(\".bloc-principal-dex:nth-child(8) .bloc-dex:nth-child(1) .description\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$poids = page %&gt;% html_nodes(\".bloc-principal-dex:nth-child(8) .bloc-dex+ .bloc-dex .description\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$niv15 = page %&gt;% html_nodes(\".bloc-principal-dex:nth-child(5) .bloc-dex:nth-child(1) .description\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  restmp$niv30 = page %&gt;% html_nodes(\".bloc-principal-dex:nth-child(5) .bloc-dex:nth-child(4) .description\") %&gt;% html_text() %&gt;% paste(collapse = \";\")\n  return(restmp)\n}\n\n\n#df &lt;- sapply(X = poke_links, FUN = get_data)\n\n\n#tdf &lt;- t(df)\n\n#tdf &lt;- as_tibble(tdf)\n\n#tdf &lt;- tdf %&gt;%\n#  mutate(numero = as.numeric(str_extract(numero, \"\\\\d+\")),\n#         nom = as.character(nom),\n#         femelle = as.numeric(str_extract(femelle, \"\\\\d+\\\\.?\\\\d*\")),\n#         PV = as.numeric(str_extract(PV, \"\\\\d+\")),\n#         ATK = as.numeric(str_extract(ATK, \"\\\\d+\")),\n#         DEF = as.numeric(str_extract(DEF, \"\\\\d+\")),\n#         taille = as.numeric(str_extract(taille, \"\\\\d+\\\\.?\\\\d*\")),\n#         poids = as.numeric(str_extract(poids, \"\\\\d+\\\\.?\\\\d*\")),\n#         niv15 = as.numeric(str_extract(niv15, \"\\\\d+\")),\n#         niv30 = as.numeric(str_extract(niv30, \"\\\\d+\")))\n\n# head(tdf, 10)"
  },
  {
    "objectID": "web_scraping.html#application-alternative-évaluations-de-ramens",
    "href": "web_scraping.html#application-alternative-évaluations-de-ramens",
    "title": "Web scraping",
    "section": "Application alternative (évaluations de ramens)",
    "text": "Application alternative (évaluations de ramens)\nLe site n’est plus organisé de la bonne façon mais les lignes de code restent intéressantes.\n\n# ramen_list &lt;- read_html(\"https://www.theramenrater.com/resources-2/the-list/\")\n\n# How the original data was (probably) created\n#ramen_reviews &lt;- ramen_list %&gt;%\n#  html_node(\"#myTable\") %&gt;%\n#  html_table() %&gt;%\n#  tbl_df() %&gt;%\n#  janitor::clean_names() %&gt;%\n#  select(-t)\n\n\n#review_links &lt;- read_html(\"https://www.theramenrater.com/resources-2/the-list/\") %&gt;%\n#  html_nodes(\"#myTable a\")\n\n#reviews &lt;- tibble(review_number = parse_number(html_text(review_links)),\n#                  link = html_attr(review_links, \"href\"))\n\n\n# page &lt;- read_html(\"https://www.theramenrater.com/2019/05/23/3180-yum-yum-moo-deng/\")\n# \n# get_review_text &lt;- function(url) {\n#   message(url)\n#   \n#   read_html(url) %&gt;%\n#     html_nodes(\".entry-content &gt; p\") %&gt;%\n#     html_text() %&gt;%\n#     str_subset(\".\")\n# }\n# \n# review_text &lt;- reviews %&gt;%\n#   head(5) %&gt;%\n#   mutate(text = map(link, possibly(get_review_text, character(0), quiet = FALSE)))"
  },
  {
    "objectID": "redressement.html",
    "href": "redressement.html",
    "title": "Redressement",
    "section": "",
    "text": "# Calculer les proportions dans la base de réponse\nproportions_reponse &lt;- enquete %&gt;%\n  group_by(genre, age, education_level, occupation_category) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  mutate(proportion_reponse = count / sum(count))\n\n# Calculer les proportions dans la base de population\nproportions_population &lt;- pop %&gt;%\n  group_by(genre, age, education_level, occupation_category) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  mutate(proportion_population = count / sum(count))\n\n# Fusionner les deux bases de données\ndonnees_redressees &lt;- proportions_reponse %&gt;%\n  left_join(proportions_population, by = c(\"genre\", \"age\", \"education_level\", \"occupation_category\"))\n\n# Calculer les facteurs de pondération\ndonnees_redressees &lt;- donnees_redressees %&gt;%\n  mutate(ponderation = proportion_population / proportion_reponse)\n\n# Appliquer les pondérations à votre base de réponse\nbase_reponse_ponderee &lt;- enquete %&gt;%\n  left_join(donnees_redressees, by = c(\"genre\", \"age\", \"education_level\", \"occupation_category\")) %&gt;%\n  mutate(ponderation = ifelse(is.na(ponderation), 1, ponderation))\n\n# Utilisez base_reponse_ponderee pour effectuer vos analyses\n\n# Exemple d'utilisation : calcul de la moyenne de certaines variables pondérées\nresultats &lt;- base_reponse_ponderee %&gt;%\n  group_by(genre) %&gt;%\n  summarise(moyenne_ponderee = weighted.mean(age, w = ponderation))\n\n# Affichez les résultats\nprint(resultats)"
  },
  {
    "objectID": "redressement.html#script-pour-redresser-un-échantillon-sur-la-base-dune-ou-plusieurs-variables",
    "href": "redressement.html#script-pour-redresser-un-échantillon-sur-la-base-dune-ou-plusieurs-variables",
    "title": "Redressement",
    "section": "",
    "text": "# Calculer les proportions dans la base de réponse\nproportions_reponse &lt;- enquete %&gt;%\n  group_by(genre, age, education_level, occupation_category) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  mutate(proportion_reponse = count / sum(count))\n\n# Calculer les proportions dans la base de population\nproportions_population &lt;- pop %&gt;%\n  group_by(genre, age, education_level, occupation_category) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  mutate(proportion_population = count / sum(count))\n\n# Fusionner les deux bases de données\ndonnees_redressees &lt;- proportions_reponse %&gt;%\n  left_join(proportions_population, by = c(\"genre\", \"age\", \"education_level\", \"occupation_category\"))\n\n# Calculer les facteurs de pondération\ndonnees_redressees &lt;- donnees_redressees %&gt;%\n  mutate(ponderation = proportion_population / proportion_reponse)\n\n# Appliquer les pondérations à votre base de réponse\nbase_reponse_ponderee &lt;- enquete %&gt;%\n  left_join(donnees_redressees, by = c(\"genre\", \"age\", \"education_level\", \"occupation_category\")) %&gt;%\n  mutate(ponderation = ifelse(is.na(ponderation), 1, ponderation))\n\n# Utilisez base_reponse_ponderee pour effectuer vos analyses\n\n# Exemple d'utilisation : calcul de la moyenne de certaines variables pondérées\nresultats &lt;- base_reponse_ponderee %&gt;%\n  group_by(genre) %&gt;%\n  summarise(moyenne_ponderee = weighted.mean(age, w = ponderation))\n\n# Affichez les résultats\nprint(resultats)"
  },
  {
    "objectID": "analyses_factorielles.html",
    "href": "analyses_factorielles.html",
    "title": "Analyses factorielles",
    "section": "",
    "text": "Ces méthodes permettent de synthétiser et de visualiser un tableau de données. Le choix de la méthode dépend du type de variables analysées."
  },
  {
    "objectID": "analyses_factorielles.html#etudier-les-individus",
    "href": "analyses_factorielles.html#etudier-les-individus",
    "title": "Analyses factorielles",
    "section": "Etudier les individus",
    "text": "Etudier les individus\nLa ressemblance entre les individus = distance (au carré) entre les individus.\nL'objectif de l'ACP est de trouver la « forme » (plan) du nuage qui résume le mieux les données. Cette forme est celle qui maximise l'inertie (dispersion ou variabilité).\nUne ACP produit 2 représentations graphiques :\n\nLe graphe des individus\nLe cercle des corrélations. Pour analyser les flèches :\n\nProximité avec la dimension permet de voir les variables qui contribuent le plus à cette dimension (ex : octobre dans la capture ci-dessous est-fortement corrélé à la première dimension et très faiblement corrélé à la seconde, car sur dim1, coordonnées Octo sont proches de 1, soit de la corrélation et proches de 0 sur la dim2).\n\n\n\n\n\nSi on prend par la suite l'exemple de juin, sur la dim2, les coordonnées sont -0.5, donc la corrélation est de -0.5. Ainsi, les villes avec des T° élevées en juin ont des coordonnées plutôt faibles sur la dim2."
  },
  {
    "objectID": "analyses_factorielles.html#etudier-les-variables",
    "href": "analyses_factorielles.html#etudier-les-variables",
    "title": "Analyses factorielles",
    "section": "Etudier les variables",
    "text": "Etudier les variables\nL'étude des variables est réalisée via la matrice de corrélation et via le cercle des corrélations. Attention, sur le cercle des corrélations, la longueur de la flèche donne la qualité de la projection de la variable. Plus est longue, plus la projection est bonne. Seules les variables bien projetées peuvent être interprétées."
  },
  {
    "objectID": "analyses_factorielles.html#aides-à-linterprétation",
    "href": "analyses_factorielles.html#aides-à-linterprétation",
    "title": "Analyses factorielles",
    "section": "Aides à l’interprétation",
    "text": "Aides à l’interprétation\n\nLe pourcentage d'inertie : donne le nombre de dimensions à interpréter. Sur une ACP, quand le pourcentage passe sous la barre des 10%, on estime qu'il n'est plus nécessaire de regarder la dimension considérée.\nPour les variables supplémentaires : on projette les barycentres des individus qui prennent cette modalités (ex : région ci-dessous).\n\n\n\n\n\n\n\nLa qualité de représentation d'une variable ou d'un individu est donnée par le cos2. Seuls les éléments bien projetés peuvent être interprétés.\nOn peut regarder la contribution d'une variable à la construction de l'axe.\nAnalyse les niveaux de corrélation entre les variables et les dimensions (ex : juin = -0.5 dans l'exemple de la partie précédente).\n\n\n\n\n\n\n\nLes ellipses de confiance\n\nPlus elles sont grandes, plus la dispersion dans le groupe d'individus étudié augmente\nL'orientation des ellipses représente la corrélation entre les composantes principales. Si l'ellipse est allongée dans une direction spécifique, cela veut dire que les individus sont fortement corrélés dans cette direction\nSi les ellipses se chevauchent, cela suggère une proximité entre les groupes d'individus (et à l'inverse, on peut identifier des groupes différents)\nPour les interpréter, il est dans tous les cas nécessaire de regarder les variables qui contribuent le plus aux dimensions étudiées."
  },
  {
    "objectID": "analyses_factorielles.html#pour-mettre-en-oeuvre-lacp",
    "href": "analyses_factorielles.html#pour-mettre-en-oeuvre-lacp",
    "title": "Analyses factorielles",
    "section": "Pour mettre en oeuvre l’ACP",
    "text": "Pour mettre en oeuvre l’ACP\n\nChoisir les variables actives\nChoisir de réduire ou non les variables (si unités différentes, on réduit toujours)\nRéaliser l'ACP (factoshiny)\nChoisir le nombre de dimensions à interpréter (inertie expliquée par les dimensions)\nInterpréter simultanément le graphe des individus et celui des variables\nUtiliser les indicateurs pour enrichir l'interprétation\nRevenir aux données brutes pour interpréter"
  },
  {
    "objectID": "analyses_factorielles.html#exercices",
    "href": "analyses_factorielles.html#exercices",
    "title": "Analyses factorielles",
    "section": "Exercices",
    "text": "Exercices\n\nExercice 1\n\n\n\n\n\nQue peut-on dire de la corrélation entre les variables :\nA-B : angle à 90°, donc cos proche de 0 = absence de corrélation\nB-C : C est mal projeté. B est bien projeté. Elles sont quasi orthogonales, donc les 2 variables sont indépendantes\nB-D : les deux variables sont corrélées négativement. Quand une augmente, l'autre diminue\nC-E : les deux variables sont mal projetées. On ne peut rien dire\nQuel est le pourcentage d'inertie associé au premier plan\nEnviron 3/5, soit 60% (car 3 variables sur les 5 sont très bien projetées).\n\n\nExercice 2\nAnalyse la qualité globale de la projection\nMême en tenant compte du faible nombre d’individus (8), la qualité globale de projection est plutôt élevée sur le plan factoriel : le pourcentage d’inertie expliquée par le plan principal est de 88%. Cela signifie que 88% de la variabilité des données est exprimée sur le plan principal. Le 1er axe en particulier est largement prépondérant (74% à lui seul).\nInterpréter les axes 1 et 2 de l'ACP\n\n\n\n\n\nLe graphe des variables montre que beaucoup de variables sont bien représentées (car la pointe des flèches est proche du cercle) et sont très corrélées avec le premier facteur.\nLes corrélations entre les variables et le facteur sont très importantes (proches de 0.9). Les variables fortement corrélées positivement au 1er facteur sont les variables qui caractérisent les bulles, ainsi que la saveur amère, saveur acide. La variable saveur alcaline est corrélée négativement avec le 1er facteur (-0.92) et est corrélée négativement avec les variables qui caractérisent les bulles.\n\n\n\n\n\nLe 1er axe oppose les eaux perçues comme très pétillantes et peu alcalines aux eaux perçues comme peu pétillantes et très alcaline. C’est donc l’aspect pétillant qui différencie le plus les eaux d’un point de vue sensoriel.\nLe 2ème axe est plutôt corrélé à la variable salée (corrélation 0.72) et oppose donc les eaux salées aux eaux non salées.\n\n\n\n\n\nL’axe 1 oppose principalement les eaux Perrier et Quézac. Perrier est très pétillante (coordonnées fortement positive), tandis que Vichy est très peu pétillante (coordonnées négatives).\nL’axe 2 oppose principalement l’eau St Yorre (particulièrement salée) aux eaux Badoit et Salvetat (très peu salées).\n\n\n\n\n\nQuelles eaux ont joué un rôle prépondérant dans la construction de l’axe 1 ? Sur quels descripteurs s’opposent les eaux situées aux deux extrémités de cet axe ?\nL’eau Perrier et l’eau de Vichy ont fortement contribué à la construction du 1er axe : 54.3% pour l’eau de Perrier (individu 8) et 33% pour l’eau de Vichy (individu 3).\n\n\n\n\n\nLes eaux Perrier et Vichy s’opposent d’un point de vue pétillant (Perrier est très pétillante alors que Vichy est peu pétillante) ainsi que du point de vue de la saveur alcaline (Vichy est très alcaline et Perrier très peu).\nLes eaux qui ont joué un rôle prépondérant dans la construction de l’axe 1 sont :\n\nPerrier (cos² = 0.95)\nVichy (cos² = 0.88)\nQuézac (cos² = 0.75)\n\nCes eaux s’opposent sur les descripteurs suivants :\n\nSaveur acide\nIntensité des émissions de bulles\nEffervescence\nIntensité du crépitement\n\nPourquoi la variable “Appréciation globale” (la note hédonique) a-t-elle été introduite en tant qu'illustrative ? À quoi sert-elle comme variable illustrative ? Est-elle bien représentée sur le plan principal ? Interpréter sa projection.\nL’appréciation globale est une variable de nature différente des descripteurs. L’appréciation globale est personnelle. Cette variable est une variable illustrative car on a souhaité positionner les eaux du point de vue de la description sensorielle. L’appréciation globale est mal représentée sur le plan, autrement dit elle n’est pas corrélée aux principales dimensions sensorielles. Certains juges apprécient les eaux pétillantes et d’autres préfèrent les eaux peu pétillantes, certains apprécient les eaux salées et d’autres préfèrent les eaux peu salées."
  },
  {
    "objectID": "stat_inf.html",
    "href": "stat_inf.html",
    "title": "Statistiques inférentielles",
    "section": "",
    "text": "On considère 2 événements A et B inclus dans l’espace de tous les possibles Ω.\n\nLa probabilité de A est compris entre 0 et 1\nLa probabilité que A ne se produise pas (l’inverse donc) est 1 – probabilité de A\nLa probabilité croisée de A et B est égale à la probabilité de A + la probabilité de B – P(A∩B)\nDeux événements sont considérés indépendants si la probabilité de A ou B (P(A∩B)) est égale à la probabilité de A * la probabilité de B\n\n\n\n\n\n\n\n\n\nOn prend un lancer de 2 D6.\n\nA = la somme des 2 dés est impaire\nB = au moins un des dés présente un 1\n\nLes ensembles qui vérifient ces conditions sont les suivants :\n\nA : (a, b) avec a et b de parités contraires\nB (1, 1), (a, 1) et (b, 1) avec a et b supérieurs ou égaux à 2\n\nDonc :\n\nP(A) = 18/36\nP(B) = 11/36\nP(A∩B) = 6/36\n\nLes deux évènements ne sont donc pas indépendants, car P(A∩B) n’est pas égale à P(A)*P(B)"
  },
  {
    "objectID": "stat_inf.html#définitions",
    "href": "stat_inf.html#définitions",
    "title": "Statistiques inférentielles",
    "section": "",
    "text": "On considère 2 événements A et B inclus dans l’espace de tous les possibles Ω.\n\nLa probabilité de A est compris entre 0 et 1\nLa probabilité que A ne se produise pas (l’inverse donc) est 1 – probabilité de A\nLa probabilité croisée de A et B est égale à la probabilité de A + la probabilité de B – P(A∩B)\nDeux événements sont considérés indépendants si la probabilité de A ou B (P(A∩B)) est égale à la probabilité de A * la probabilité de B"
  },
  {
    "objectID": "stat_inf.html#exemples",
    "href": "stat_inf.html#exemples",
    "title": "Statistiques inférentielles",
    "section": "",
    "text": "On prend un lancer de 2 D6.\n\nA = la somme des 2 dés est impaire\nB = au moins un des dés présente un 1\n\nLes ensembles qui vérifient ces conditions sont les suivants :\n\nA : (a, b) avec a et b de parités contraires\nB (1, 1), (a, 1) et (b, 1) avec a et b supérieurs ou égaux à 2\n\nDonc :\n\nP(A) = 18/36\nP(B) = 11/36\nP(A∩B) = 6/36\n\nLes deux évènements ne sont donc pas indépendants, car P(A∩B) n’est pas égale à P(A)*P(B)"
  },
  {
    "objectID": "stat_inf.html#loi-de-bernoulli",
    "href": "stat_inf.html#loi-de-bernoulli",
    "title": "Statistiques inférentielles",
    "section": "Loi de Bernoulli",
    "text": "Loi de Bernoulli\nCette loi s’applique dans les cas où deux résultats sont possibles lors d’un événement. Par exemple : tirer une boule noire ou une boule blanche.\nNous prenons la variable aléatoire X suivante :\n\nX = 1 : tirage d’une boule blanche\nX = 0 : tirage d’une boule noire\n\nDans ce cas :\n\nP(X = 1) = p\nP(X = 0) = 1 – p\n\nOn dit que la variable aléatoire suit une loi de Bernoulli de paramètre p compris entre 0 et 1 :"
  },
  {
    "objectID": "stat_inf.html#loi-binomiale",
    "href": "stat_inf.html#loi-binomiale",
    "title": "Statistiques inférentielles",
    "section": "Loi binomiale",
    "text": "Loi binomiale\nLa loi binomiale est la loi que suit une série de lois de Bernoulli (somme de n Bernoulli). Par exemple : série de n tirages.\nSi nous prenons la variable aléatoire X = nombre de boules blanches tirées à l’issue de ces n tirages. On dit que cette variable aléatoire suit une loi binomiale de paramètres n (nombre de tirages) et p (probabilité de l’événement étudié)."
  },
  {
    "objectID": "stat_inf.html#loi-uniforme",
    "href": "stat_inf.html#loi-uniforme",
    "title": "Statistiques inférentielles",
    "section": "Loi uniforme",
    "text": "Loi uniforme\n\n\n\n\n\nUne loi est uniforme si chaque issue à une probabilité égale de se produire."
  },
  {
    "objectID": "stat_inf.html#loi-normale",
    "href": "stat_inf.html#loi-normale",
    "title": "Statistiques inférentielles",
    "section": "Loi normale",
    "text": "Loi normale\nLa loi normale N (gaussienne) suit les paramètres :\n\nµ = espérance (soit la moyenne)\nσ² = écart-type\n\nLa loi N(0,1) est appelée loi normale centrée réduite.\nΦ désigne la fonction de répartition d’une loi normale N(0,1).\n\n\n\n\n\nD’autres exemples de lois continues :\n\nloi du Khi-deux\nloi de Fischer\nloi de Student"
  },
  {
    "objectID": "stat_inf.html#estimations-ponctuelles-réponse-à-la-q1",
    "href": "stat_inf.html#estimations-ponctuelles-réponse-à-la-q1",
    "title": "Statistiques inférentielles",
    "section": "Estimations ponctuelles (réponse à la Q1)",
    "text": "Estimations ponctuelles (réponse à la Q1)\nPour appréhender les paramètres des lois des cas 1 et 2, on ne dispose que des observations, réalisations des variables aléatoires iid. On utilise des estimateurs :\n\nPour les lois de Bernoulli : c’est la proportion / fréquence empirique(ex : fréquence de guérisons, soit 167/216, soit 0.773)\nPour les lois normales :\n\nL’estimateur de l’espérance est la moyenne\nL’estimateur de la variance est la variance empirique"
  },
  {
    "objectID": "stat_inf.html#intervalles-de-confiance-réponse-à-la-q2",
    "href": "stat_inf.html#intervalles-de-confiance-réponse-à-la-q2",
    "title": "Statistiques inférentielles",
    "section": "Intervalles de confiance (réponse à la Q2)",
    "text": "Intervalles de confiance (réponse à la Q2)\nL’estimateur est un estimateur. Il varie en fonction de l’échantillon considéré. Il est souvent associé à un intervalle de confiance, qui est permet d’énoncer des phrases du type « Il y a 95% de chances pour que le paramètre théorique se trouve dans cet intervalle ».\nOn peut construire des intervalles de confiance bilatères (encadre le paramètre à gauche et à droite) ou unilatères (encadre le paramètre à gauche ou à droite).\nClassiquement, les intervalles de confiance sont de niveaux 90% (α = 10%) ou 95% (α = 5%).\n\nLes intervalles de confiance pour les proportions\nConditions d’application :\n\nPour les petits échantillons (n &lt; 30), on se base sur la loi binomiale\nPour les grands échantillons ou si la proportion est inférieure ou égale à 5, on utilise l’approximation de la loi normale.\n\nPour le cas 1 :\n\nAvec un intervalle de confiance de 95%, on obtient que la proportion théorique est comprise entre 0.717 et 0.829.\nLa largeur de l’intervalle n’est pas négligeable. Mais il n’y a que 216 individus dans l’échantillon.\n\n\nprop.test(x = 167, n = 216, p = 0.75, alternative = \"greater\", conf.level = 0.95) # Argument conf.level et regarder la sortie \"95 percent confidence interval\"\n\n\n    1-sample proportions test with continuity correction\n\ndata:  167 out of 216, null probability 0.75\nX-squared = 0.5, df = 1, p-value = 0.2398\nalternative hypothesis: true p is greater than 0.75\n95 percent confidence interval:\n 0.7206161 1.0000000\nsample estimates:\n        p \n0.7731481 \n\n\n\n\nLes intervalles de confiance pour les moyennes\nConditions d’application :\n\nOn a une loi normale\nL’échantillon est supérieur à 30\n\nPour le cas 2 :\n\nPour un intervalle de confiance de 95%, on obtiens que la moyenne théorique est comprise entre 31.07 et 31.83.\n\n\nt.test(x = iris$Sepal.Length, mu = 5.7, alternative = \"greater\", conf.level = 0.95) # Argument conf.level et regarder la sortie \"95 percent confidence interval\"\n\n\n    One Sample t-test\n\ndata:  iris$Sepal.Length\nt = 2.12, df = 149, p-value = 0.01783\nalternative hypothesis: true mean is greater than 5.7\n95 percent confidence interval:\n 5.731427      Inf\nsample estimates:\nmean of x \n 5.843333 \n\n\n\n\nLes intervalles de confiance pour les variances\nConditions d’application :\n\nOn a une loi normale\nL’échantillon est supérieur à 30\n\nPour le cas 2 :\n\nPour un intervalle de confiance de 95%, on obtiens que la variance théorique est comprise entre 3.70 et 6.06."
  },
  {
    "objectID": "stat_inf.html#tests-statistiques",
    "href": "stat_inf.html#tests-statistiques",
    "title": "Statistiques inférentielles",
    "section": "Tests statistiques",
    "text": "Tests statistiques\nOn veut savoir si le taux de guérison du médicament est meilleur que pour l'ancien médicament. L'efficacité de ce nouveau médicament est considérée par défaut comme au mieux similaire à celle du précédent médicament. Cet a priori correspond à ce que appelle l'hypothèse nulle, appelée H0. L'hypothèse alternative est notée H1. Elle permet d'indiquer les cas de figure où on rejette H0.\nOn pose ainsi 2 hypothèses :\n\nH0 : p = p0\nH1 : p &gt; p0\n\nLe risque de première espèce correspond au risque de rejeter H0 à tort.\nLe niveau de test est la région de rejet de H0 (5%, 1%, …)\nLe risque de seconde espèce correspond au risque de non-rejet de H0 à mauvais escient.\n\nL'hypothèse nulle H0 est l'hypothèse qu'on ne souhaite pas rejeter trop facilement. C'est celle que l'on teste.\nL'hypothèse alternative, H1, indique dans quelles conditions on rejette H0. Elle donne donc des informations sur la forme de la région critique. (basée sur une statistique de test). Pour déterminer la région critique, il faut connaitre la loi de la statistique de test sous H0.\n\nPour déterminer la région critique, on regarde la p-valeur :\n\nRejet de H0 au niveau de test α ⬄ p-valeur &lt; α\n\nSi la p-valeur est faible, cela signifie que l'observation est rare (donc peu probable) sous H0, ce qui incite à rejeter H0.\n\nTest paramétrique pour une proportion\n\n\n\n\n\nIci, p0 = 0.75\n\n\n\n\n\n\n\n\n\n\nOn peut utiliser la fonction\n\nProp.test() – test asymptotique avec la loi du khi-deux\nBinom.test() – test exact\n\nOn peut choisir si le test est unilatère ou bilatère avec le paramètre « alternative » : greater, lesser ou two-sided.\nIl faut regarder la « p-value », qui donne le niveau de risque pour rejeter l’hypothèse H0.\n\n\nTest paramétrique pour une moyenne\n\n\n\n\n\nIl faut utiliser la fonction t.test dans R.\n\n\nTest paramétrique pour une variance\nIl faut utiliser la fonction var.test dans R."
  },
  {
    "objectID": "stat_inf.html#intervalle-de-confiance-dune-moyenne",
    "href": "stat_inf.html#intervalle-de-confiance-dune-moyenne",
    "title": "Statistiques inférentielles",
    "section": "Intervalle de confiance d’une moyenne",
    "text": "Intervalle de confiance d’une moyenne\nNous cherchons à construire un intervalle de confiance pour la moyenne mu d'une variable quantitative. En pratique, pour un petit échantillon, on teste la normalité des données avant d'appliquer la formule. Pour les grands échantillons (n &gt; 30), le théorème central limite assure que la formule reste satisfaisante.\nPour ce faire, il faut suivre les étapes suivantes :\n\nImporter les données\nEstimer les paramètres (moyenne et écart-type)\nReprésenter la distribution des données. Avant toute analyse, il est conseillé de représenter les données (histogramme).\nConstruire l'intervalle de confiance\n\nPour ce faire, on utilise la fonction t.test(), avec le « conf.level » attendu. Avec ce résultat, nous pouvons affirmer, avec une confiance de 95%, dans l'intervalle obtenu contient la « vraie » valeur de mu.\nSi nous réalisons ce test sur une proportion et non sur une moyenne, il faut utiliser la fonction prop.test()."
  },
  {
    "objectID": "stat_inf.html#test-du-chi²-dindépendance",
    "href": "stat_inf.html#test-du-chi²-dindépendance",
    "title": "Statistiques inférentielles",
    "section": "Test du chi² d’indépendance",
    "text": "Test du chi² d’indépendance\nIl faut utiliser la fonction chisq.test() pour tester l'indépendance entre les effectifs de deux variables qualitatives (l'exemple qui est donné est celui de la répartition de la couleur de cheveux en fonction du sexe). Le test permettra de conclure (ou non) que la couleur de cheveux dépend du sexe de l'enfant.\nLe test se construit sur la base d'un tableau de contingence qui peut être obtenu par la fonction xtabs()."
  },
  {
    "objectID": "stat_inf.html#comparaison-de-2-moyennes",
    "href": "stat_inf.html#comparaison-de-2-moyennes",
    "title": "Statistiques inférentielles",
    "section": "Comparaison de 2 moyennes",
    "text": "Comparaison de 2 moyennes\nIl faut disposer d'une variable quantitative qui est supposée avoir été tirée aléatoirement dans une population.\nH0 = les moyennes théoriques mu1 et mu2 sont égales\nH1 = les moyennes théoriques mu1 et mu2 ne sont pas égales.\nL'exemple est celui du poids des poulpes mâles et femelles au stade adulte. Attention, dans le cas des petits échantillons (n &lt; 30), il faut tester l'hypothèse de normalité.\nLa comparaison se réalise en suivant les étapes suivantes :\n\nImporter les données\nComparer graphiquement les deux populations (comparaison de boxplots)\nEstimer les statistiques de base dans chaque groupe\n\nPour ce faire, dans R, on peut utiliser la fonction aggregate et summary. Exemple :\naggregate(don$Poids, by = list(don$Sexe), FUN = summary)\nPour obtenir l'écart-type, on peut utiliser la fonction tapply :\ntapply(don$Poids, don$Sexe, sd)\n\nTester la normalité des données\n\nLe test ne doit être réalisé que si la taille de l'échantillon est inférieure à 30.\nLe test est réalisé en mobilisant le test de Shapiro-Wilk (shapiro.test). Si la probabilité critique est supérieure à 5%, on conserve l'hypothèse de normalité.\n\nTester l'égalité des variances\n\nOn utilise la fonction var.test.\n\nTester l'égalité des moyennes\n\nOn peut utiliser la fonction t.test en renseignant l'argument var.equal = FALSE. Dans ce cas, c'est le test de Welsh. Les modalités de rédaction sont les suivantes :\nt.test(Poids ~ Sexe, alternative = « two.sided », conf.level = 0.95, var.equal = FALSE, data = don)"
  },
  {
    "objectID": "stat_inf.html#test-sur-les-proportions",
    "href": "stat_inf.html#test-sur-les-proportions",
    "title": "Statistiques inférentielles",
    "section": "Test sur les proportions",
    "text": "Test sur les proportions\nIl s'agit des tests :\n\nDe conformité d'une proportion à une valeur donnée. Dans ce cas, on cherche la probabilité d'observer un caractère particulier dans une population et on examine l'hypothèse d'égalité de cette probabilité à une valeur donnée\nDes tests d'égalité de plusieurs proportions. Dans ce cas, on cherche à comparer les probabilités d'observation d'un caractère particulier selon l'appartenance des individus à différentes catégories.\n\nOn reprend l'exemple de la répartition des couleurs de cheveux en fonction du sexe de l'enfant. On s'intéresse à la répartition des cheveux blonds en fonction du sexe :\n\nDans un premier temps, on cherche à tester l'égalité des proportions de filles et garçons avec les cheveux blonds. Pour ce faire, on utilise la fonction binom.test, qui réalise un test binomial exact.\nDans un second temps, on cherche à tester l'égalité de la proportion des garçons dans les 5 catégories définies par la couleur de cheveux (blond, roux, brun, …). Pour ce faire, on utilise la fonction prop.test.\n\nCas 1 :\nOn veut tester si la proportion de garçons avec les cheveux blonds est égale à 50% :\nH0 = la proportion est égale à 50%\nH1 = la proportion n'est pas égale à 50%\n\nbinom.test(x = 592, n = 1136, p = 0.5, alternative = \"two.sided\")\n\n\n    Exact binomial test\n\ndata:  592 and 1136\nnumber of successes = 592, number of trials = 1136, p-value = 0.1631\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4916142 0.5505297\nsample estimates:\nprobability of success \n             0.5211268 \n\n\nSi la probabilité critique est plus élevée que 5%, nous conservons l'hypothèse H0 pour ce seuil.\nCas 2 :\nOn veut tester l'égalité des proportions de filles pour les différentes couleurs de cheveux. Pour ce faire, on utilise la fonction prop.test :\n\nprop.test(c(544, 97, 677, 451, 14), n=c(1136, 216, 1526, 955, 50))\n\n\n    5-sample test for equality of proportions without continuity correction\n\ndata:  c(544, 97, 677, 451, 14) out of c(1136, 216, 1526, 955, 50)\nX-squared = 10.467, df = 4, p-value = 0.03325\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4    prop 5 \n0.4788732 0.4490741 0.4436435 0.4722513 0.2800000 \n\n\nSi la p-valeur est inférieure à 5%, on peut rejeter l'hypothèse d'égalité."
  },
  {
    "objectID": "regression_linéaire.html",
    "href": "regression_linéaire.html",
    "title": "Régression linéaire",
    "section": "",
    "text": "L'objectif de la régression est de construire un modèle permettant de modéliser les relations entre plusieurs variables, dans un but d'analyse ou afin de réaliser des prédictions ou des prévisions. Les objectifs sont doubles :\n\nExpliquer une variable Y par des variables X, et donc mettre au jour les liens entre la variable Y et ces variables ;\nPrédire de nouvelles valeurs pour Y.\n\nDans le cas de la régression linéaire, il s'agit de chercher une fonction f telle que :\n\n\n\n\n\nDans cette exemple, il s'agit d'une fonction pour prédire un pic d'ozone(O3) en fonction de la température (T12)\nLe modèle linéaire est pertinent si la relation entre les variables est linéaire (et non sinusoïdale ou autre).\nAfin de déterminer cette fonction, il faut choisir un critère appelé fonction de coût. Nous choisissons le coût quadratique. Notre droite sera donc la droite des moindres. Cela permet d'éviter d'avoir de très fortes erreurs dans le modèle, puisqu'elles seront élevées au carré (et donc facilement identifiables).\nDans le cas présent, les minimiseurs sont Beta et Beta2.\n\n\n\n\n\nDans le cas du modèle prédictif de la température, l'équation est donc la suivante :\n\n\n\n\n\nAvant de lancer une régression, il peut être intéressant de regarder les coefficients de corrélation entre les variables explicatives et la variable à expliquer. Si tous les coefficients sont faibles à priori le modèle ne sera pas très bon.\n\n\nNous considérons le modèle suivant :\n\n\n\n\n\nIci :\n\nXi (la température T12) est fixée (connue)\nEpsiloni (erreur due à l'imprécision modèle & mesure) est aléatoire\nYi (l'ozone O3) est aléatoire\nBeta1 et Beta2 sont fixes et inconnus.\n\nL'objectif du modèle est de fournir des estimateurs de Beta1 et Beta2.\nLes estimateurs des moindres carrés sont les suivants :\n\n\n\n\n\nUne fois ces éléments posés, nous pouvons nous poser les questions suivantes :\n\nQuelles sont les propriétés des estimateurs ?\nQuelles est la variabilité / précision de l'estimateur ?\nLe modèle est-il bon ?\nQuelle est la variabilité de la prévision ?\n\nDans R :\n\nLa création d'un modèle linéaire est réalisé avec la fonction lm().\nLa variable à expliquer (Y) est placée en avant, les variables explicatives par la suite, séparées de la variable à expliquer par le ~. Si on prend toutes les variables contenues dans le data frame, on peut utiliser le « . ».\nLa constante Beta1 est toujours sous-entendue. Pour la retirer, il est faut ajouter -1 devant les variables explicatives.\n\nLa lecture des résultats de la fonction est réalisée de la façon suivante.\nCall : rappel de la formule du modèle\nCoefficients :\n\nEstimate : estimation de l'estimateur\nStd. Error : écart-type de l'estimateur\nT value : estimation divisée par l'écart-type. Il faut qu'elle soit supérieure à |2| pour que la variable ait un effet dans le modèle\nPr (&gt; |t|) : résultat du test statistique. Il permet de déterminer si on refuse l'hypothèse que les estimateurs (Beta1 ou Beta2 ici) vaillent 0. Dans le cas présent, nous rejetons cette hypothèse dans les 2 cas, car le résultat est inférieur à 5% (0.05).\n\n\n\n\nLa construction d'un modèle suppose de réaliser les étapes suivantes :\n\nOn mesure des variables – on construit un tableau de données\nDans ces variables, on identifie une variable à expliquer et une (ou plusieurs) variables (potentiellement) explicatives\nOn suppose que la variable à expliquer suive la fonction suivante :\n\ny = Beta1 + Beta2 * variableExp + épsilon (bruit)\n\nOn cherche à estimer les paramètres Beta1 et Beta2. Pour ce faire, on détermine la fonction de coût que l'on souhaite appliquer (ici, moindres carrés ordinaires)\nOn utilise la fonction lm() pour obtenir les estimateurs, la variance de chaque coefficient et la statistique de test (si H0 est vérifiée, le paramètre est égal à 0)"
  },
  {
    "objectID": "regression_linéaire.html#principes-de-modélisation",
    "href": "regression_linéaire.html#principes-de-modélisation",
    "title": "Régression linéaire",
    "section": "",
    "text": "Nous considérons le modèle suivant :\n\n\n\n\n\nIci :\n\nXi (la température T12) est fixée (connue)\nEpsiloni (erreur due à l'imprécision modèle & mesure) est aléatoire\nYi (l'ozone O3) est aléatoire\nBeta1 et Beta2 sont fixes et inconnus.\n\nL'objectif du modèle est de fournir des estimateurs de Beta1 et Beta2.\nLes estimateurs des moindres carrés sont les suivants :\n\n\n\n\n\nUne fois ces éléments posés, nous pouvons nous poser les questions suivantes :\n\nQuelles sont les propriétés des estimateurs ?\nQuelles est la variabilité / précision de l'estimateur ?\nLe modèle est-il bon ?\nQuelle est la variabilité de la prévision ?\n\nDans R :\n\nLa création d'un modèle linéaire est réalisé avec la fonction lm().\nLa variable à expliquer (Y) est placée en avant, les variables explicatives par la suite, séparées de la variable à expliquer par le ~. Si on prend toutes les variables contenues dans le data frame, on peut utiliser le « . ».\nLa constante Beta1 est toujours sous-entendue. Pour la retirer, il est faut ajouter -1 devant les variables explicatives.\n\nLa lecture des résultats de la fonction est réalisée de la façon suivante.\nCall : rappel de la formule du modèle\nCoefficients :\n\nEstimate : estimation de l'estimateur\nStd. Error : écart-type de l'estimateur\nT value : estimation divisée par l'écart-type. Il faut qu'elle soit supérieure à |2| pour que la variable ait un effet dans le modèle\nPr (&gt; |t|) : résultat du test statistique. Il permet de déterminer si on refuse l'hypothèse que les estimateurs (Beta1 ou Beta2 ici) vaillent 0. Dans le cas présent, nous rejetons cette hypothèse dans les 2 cas, car le résultat est inférieur à 5% (0.05)."
  },
  {
    "objectID": "regression_linéaire.html#logique-de-mise-en-application",
    "href": "regression_linéaire.html#logique-de-mise-en-application",
    "title": "Régression linéaire",
    "section": "",
    "text": "La construction d'un modèle suppose de réaliser les étapes suivantes :\n\nOn mesure des variables – on construit un tableau de données\nDans ces variables, on identifie une variable à expliquer et une (ou plusieurs) variables (potentiellement) explicatives\nOn suppose que la variable à expliquer suive la fonction suivante :\n\ny = Beta1 + Beta2 * variableExp + épsilon (bruit)\n\nOn cherche à estimer les paramètres Beta1 et Beta2. Pour ce faire, on détermine la fonction de coût que l'on souhaite appliquer (ici, moindres carrés ordinaires)\nOn utilise la fonction lm() pour obtenir les estimateurs, la variance de chaque coefficient et la statistique de test (si H0 est vérifiée, le paramètre est égal à 0)"
  },
  {
    "objectID": "regression_linéaire.html#présentation-du-cas-détude-et-modélisation",
    "href": "regression_linéaire.html#présentation-du-cas-détude-et-modélisation",
    "title": "Régression linéaire",
    "section": "Présentation du cas d’étude et modélisation",
    "text": "Présentation du cas d’étude et modélisation\nDans le cas de la régression linéaire multiple (ou classification supervisée quand la variable à expliquer est qualitative), plusieurs variables explicatives sont mobilisées pour expliquer une variable Y.\n\n\n\n\n\nL'écriture matricielle est la suivante :\n\n\n\n\n\nS'agissant de l'erreur epsilon, nous faisons l'hypothèse que son espérance vaut 0 et que sa variance est égale à son écart-type."
  },
  {
    "objectID": "regression_linéaire.html#estimation",
    "href": "regression_linéaire.html#estimation",
    "title": "Régression linéaire",
    "section": "Estimation",
    "text": "Estimation\nLa fonction de coût est les moindres carrés ordinaires. Nous cherchons donc la fonction qui minime le « coût ». Celle-ci permet de définir des estimateurs appelés Beta chapeau (chapeau car ils sont estimés au regard des données à notre disposition).\nLes résultats sont :\n\nL'estimation des coefficients\nLes valeurs ajustées (fitted values)\nLes résidus (residuals)"
  },
  {
    "objectID": "regression_linéaire.html#ajustement-et-prévision",
    "href": "regression_linéaire.html#ajustement-et-prévision",
    "title": "Régression linéaire",
    "section": "Ajustement et prévision",
    "text": "Ajustement et prévision\nA partir de nos estimateurs, on peut donner une prévision. Celle-ci est néanmoins toujours accompagnée d'un intervalle de confiance (et donc déterminer la variabilité du modèle)."
  },
  {
    "objectID": "regression_linéaire.html#variabilité",
    "href": "regression_linéaire.html#variabilité",
    "title": "Régression linéaire",
    "section": "Variabilité",
    "text": "Variabilité\nPlusieurs dimensions peuvent être étudiées afin de déterminer la précision du modèle :\n\nL'espérance et la variance de Beta chapeau\nL'intervalle de confiance de Beta i\nL'intervalle de confiance de Y*\n\nY* est une variable aléatoire. L'objectif du modèle est d'avoir un écart faible entre Y* - Y* chapeau."
  },
  {
    "objectID": "regression_linéaire.html#qualité-du-modèle",
    "href": "regression_linéaire.html#qualité-du-modèle",
    "title": "Régression linéaire",
    "section": "Qualité du modèle",
    "text": "Qualité du modèle\nPlusieurs éléments peuvent être mobilisés afin de déterminer la qualité du modèle :\n\nLe R². Il est compris entre 0 et 1. C'est une première mesure mais elle est peu fiable : le R² augmente mécaniquement lorsque le nombre de variables augmente.\nRegarder les résidus studentisés. Ces derniers permettent de déterminer si une observation qu'on réalise « arrive » souvent ou non, c'est-à-dire les points aberrants. On peut les identifier via les résidus studentisés du modèle (rstudent(modele$residuals))\nIdentifier les points leviers, via la fonction hatvalues()."
  },
  {
    "objectID": "regression_linéaire.html#ajustement-et-prévision-1",
    "href": "regression_linéaire.html#ajustement-et-prévision-1",
    "title": "Régression linéaire",
    "section": "Ajustement et prévision",
    "text": "Ajustement et prévision\nRéaliser une prévision, c'est étendre ses conclusions à autre chose que l'échantillon étudié. Plusieurs méthodes peuvent être mobilisées :\n\nL'apprentissage-validation : On sépare notre échantillon en deux groupes pour tester le modèle (groupe pour l'apprentissage et groupe pour la validation)\nLa validation croisée : On sépare notre échantillon en groupes et on teste de façon croisée le modèle"
  },
  {
    "objectID": "regression_linéaire.html#choix-des-variables-choix-des-modèles",
    "href": "regression_linéaire.html#choix-des-variables-choix-des-modèles",
    "title": "Régression linéaire",
    "section": "Choix des variables, choix des modèles",
    "text": "Choix des variables, choix des modèles\nLe choix du modèle est réalisé par la méthode AIC ou BIC. La sélection des variables peut être :\n\nForward : on ajoute un coefficient à chaque étape, tant que cela ne dégrade pas l'indice AIC\nBackward : on retire un coefficient à chaque étape, tant que cela ne dégrade pas l'indice AIC\nSideways : on ajoute un coefficient à chaque étape, avec une possibilité de faire « machine arrière »\n\nNous retenons le plus souvent la modalité backward."
  },
  {
    "objectID": "regression_linéaire.html#application",
    "href": "regression_linéaire.html#application",
    "title": "Régression linéaire",
    "section": "Application",
    "text": "Application\nD'un point de vue logiciel, la séquence est ainsi la suivante :\n\nOn charge les données\nOn déterminer la variable à expliquer et les variables explicatives\nOn peut réaliser une ACP pour étudier la structure de corrélation et identifier les individus leviers\nOn réalise un premier modèle\nOn regarde la significativité des paramètres (test statistique)\nOn retire les variables non significatives du modèle, jusqu'à obtenir le modèle final\nOn regarde les points aberrants via les résidus studentisés (structure via représentation graphique ou identification des grands résidus).\nOn retire les aberrants et on refait un modèle par itération."
  },
  {
    "objectID": "regression_logistique.html",
    "href": "regression_logistique.html",
    "title": "Régression logistique",
    "section": "",
    "text": "La régression logistique est une méthode permettant de prédire ou d’expliquer une variable qualitative qui prend 2 valeurs (Oui / Non, …).\n\n\n\n\n\n\n\n\n\nRégression linéaire\nRégression logistique\n\n\n\n\nVariable à expliquer\nQuantitative continue\nQualitative sur 2 positions\n\n\nEstimation des coefficients\nPar minimisation de la fonction de coût\nPar maximisation de la vraisemblance (via un algorithme itératif)\n\n\nRésidus\nRésidus studentisés\nRésidus de déviance (les plus courants)\nRésidus de Pearson\n\n\nPoints leviers\nOui\nOui\n\n\n\nDans R :\n\nIl faut utiliser la fonction glm() avec le paramètre “family = binomial”\n\nDans le cas de la mise en place d’un modèle de prévision, nous devons déterminer sa capacité prédictive. Il faut affecter les Y estimés / prévus pour déterminer les :\n\nVrais positifs\nFaux positifs\nVrais négatifs\nFaux négatifs\n\nOn calcule sur cette base :\n\nPrécision : nb de fois où on a raison / nb total de cas =&gt; ne permet pas de préciser si on se trompe sur les 0 ou sur les 1\nSensibilité : nb de VP / nb de 1, donc le pourcentage de vrais positifs\nSpécificité : nb de VN / nb de négatifs\n\nPour ce faire, il faut déterminer un seuil :\n\nSeuil de Bayes (0.5)\nSeuil naturel\n\nLa recherche du seuil est réalisée via le script suivant :\n\n# seuilc &lt;- sort(prevc)[floor(nrow(donT)*0.65)+1]\n# seuilaic &lt;- sort(prevAIC)[floor(nrow(donT)*0.65)+1]\n# seuilbic &lt;- sort(prevBIC)[floor(nrow(donT)*0.65)+1]"
  },
  {
    "objectID": "regression_logistique.html#validation-croisée-par-blocs-régression-linéaire",
    "href": "regression_logistique.html#validation-croisée-par-blocs-régression-linéaire",
    "title": "Régression logistique",
    "section": "Validation croisée par blocs (régression linéaire)",
    "text": "Validation croisée par blocs (régression linéaire)\nLe script ci-dessous permet de faire de la validation croisée par blocs :\n\n# nb=10 # nombre de groupes\n# \n# set.seed(1234)\n# \n# bloc &lt;- sample(rep(1:nb,length=nrow(don)))\n# RES &lt;- data.frame(Y=don$Y)\n# \n# for(ii in 1:nb){\n#   print(ii)\n#   donA=don[bloc!=ii,]\n#   donT=don[bloc==ii,]\n#   modcomplet &lt;- lm(Y~.,data=donA)\n#   modAIC &lt;- step(modcomplet,trace=0)\n#   modBIC &lt;- step(modcomplet,k=log(nrow(donA)),trace=0)\n#   RES[bloc==ii,\"complet\"] &lt;- predict(modcomplet,donT)\n#   RES[bloc==ii,\"aic\"] &lt;- predict(modAIC,donT)\n#   RES[bloc==ii,\"bic\"] &lt;- predict(modBIC,donT)\n# }\n# erreur &lt;- function(X,Y){mean((X-Y)^2)}\n# apply(RES,2,erreur,Y=RES$Y)"
  },
  {
    "objectID": "regression_logistique.html#validation-croisée-par-blocs-régression-logistique",
    "href": "regression_logistique.html#validation-croisée-par-blocs-régression-logistique",
    "title": "Régression logistique",
    "section": "Validation croisée par blocs (régression logistique)",
    "text": "Validation croisée par blocs (régression logistique)\n\n# nb &lt;- 10 # nombre de groupes\n# \n# set.seed(1234)\n# \n# bloc &lt;- sample(rep(1:nb,length=nrow(don)))\n# RES &lt;- data.frame(Y=don$Y,log=NA,aic=NA,bic=NA)\n# \n# for(ii in 1:nb){\n#   donA &lt;- don[bloc!=ii,]\n#   donT &lt;- don[bloc==ii,]\n#   ############################################\n#   algo1 &lt;- glm(Y~.,data=donA,family=\"binomial\")\n#   RES[bloc==ii,\"log\"] &lt;- predict(algo1,donT,type=\"response\")\n#   ############################################\n#   algo2 &lt;- step(algo1,trace=0)\n#   RES[bloc==ii,\"aic\"] &lt;- predict(algo2,donT,type=\"response\")\n#   ############################################\n#   algo3 &lt;- step(algo1,trace=0,k=log(nrow(donA)))\n#   RES[bloc==ii,\"bic\"] &lt;- predict(algo3,donT,type=\"response\")\n# }\n\nLa validation du modèle est réalisée avec le script suivant :\n\n# library(pROC)\n# rocp &lt;- roc(Y~.,data=resdf) # resdf = df obtenu via l'aglo précédent\n# sapply(rocp,coords,x=0.5,ret=c(\"accuracy\",\"threshold\"))"
  },
  {
    "objectID": "geolocalisation.html",
    "href": "geolocalisation.html",
    "title": "Géolocalisation",
    "section": "",
    "text": "# # Installer le package osrm si ce n'est pas déjà fait\n# # install.packages(\"osrm\")\n# \n# # Charger la bibliothèque\n# library(osrm)\n# \n# # Créer un objet pour stocker les résultats\n# res &lt;- list()\n# \n# # Utiliser une boucle for pour itérer sur les lignes des données\n# for (ii in 1:nrow(data_departs)) {\n#   print(paste(\"Lecture de la ligne \", ii))\n#   \n#   # Obtenir les coordonnées de départ et d'arrivée en tant que vecteurs\n#   coord_depart &lt;- unlist(data_departs[ii, c(\"longitude\", \"latitude\")])\n#   coord_arrivee &lt;- unlist(data_arrivees[ii, c(\"longitude\", \"latitude\")])\n#   \n#   # Effectuer la requête OSRM\n#   res[[ii]] &lt;- osrmRoute(src = coord_depart, dst = coord_arrivee, osrm.profile = \"car\")\n#   \n#   # Attendre 1 seconde entre les requêtes (pour éviter les limitations)\n#   Sys.sleep(2)\n# }\n# \n# # Vous pouvez maintenant accéder aux résultats dans la liste res\n# # Par exemple, le résultat de la première requête est dans res[[1]]\n# \n# resultats &lt;- bind_rows(res) %&gt;%\n#   as_tibble()"
  },
  {
    "objectID": "geolocalisation.html#méthode-pour-calculer-la-distance-et-le-temps-de-trajet-entre-2-adresses-géolocalisées",
    "href": "geolocalisation.html#méthode-pour-calculer-la-distance-et-le-temps-de-trajet-entre-2-adresses-géolocalisées",
    "title": "Géolocalisation",
    "section": "",
    "text": "# # Installer le package osrm si ce n'est pas déjà fait\n# # install.packages(\"osrm\")\n# \n# # Charger la bibliothèque\n# library(osrm)\n# \n# # Créer un objet pour stocker les résultats\n# res &lt;- list()\n# \n# # Utiliser une boucle for pour itérer sur les lignes des données\n# for (ii in 1:nrow(data_departs)) {\n#   print(paste(\"Lecture de la ligne \", ii))\n#   \n#   # Obtenir les coordonnées de départ et d'arrivée en tant que vecteurs\n#   coord_depart &lt;- unlist(data_departs[ii, c(\"longitude\", \"latitude\")])\n#   coord_arrivee &lt;- unlist(data_arrivees[ii, c(\"longitude\", \"latitude\")])\n#   \n#   # Effectuer la requête OSRM\n#   res[[ii]] &lt;- osrmRoute(src = coord_depart, dst = coord_arrivee, osrm.profile = \"car\")\n#   \n#   # Attendre 1 seconde entre les requêtes (pour éviter les limitations)\n#   Sys.sleep(2)\n# }\n# \n# # Vous pouvez maintenant accéder aux résultats dans la liste res\n# # Par exemple, le résultat de la première requête est dans res[[1]]\n# \n# resultats &lt;- bind_rows(res) %&gt;%\n#   as_tibble()"
  }
]